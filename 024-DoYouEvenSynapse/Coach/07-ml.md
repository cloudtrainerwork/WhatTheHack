# Challenge 7: Leverage Machine Learning

This challenge builds on previous ones by introducing machine learning (ML) in Azure Synapse Analytics to analyze the Open Powerlifting dataset. The focus is on using Apache Spark pools to apply clustering algorithms on lifters' raw performance metrics (e.g., squat, bench press, deadlift, total lifted, bodyweight) to identify potential patterns or groupings. This could reveal if current weight classes are optimal or suggest more appropriate ones based on performance clusters rather than just bodyweight. Clustering helps uncover hidden structures in data, such as grouping lifters with similar strength profiles.

We'll use PySpark in a Synapse Spark pool, leveraging MLlib (Spark's ML library) for K-Means clustering—a simple, scalable algorithm for unsupervised learning. If needed, SynapseML can be explored for advanced features, but MLlib suffices here. Emphasize distributed computing benefits for large datasets like Open Powerlifting (millions of rows).

As a coach, demonstrate Spark pool setup, code execution in notebooks, and result interpretation. Highlight why Spark: Handles big data efficiently, integrates with ADLS, and supports ML at scale. Discuss ethical considerations, like using masked data from Challenge 5 to protect PII.

The student guide should include code snippets and prompts for experimentation (e.g., try different cluster numbers).

## Prerequisites
- Azure Synapse Workspace with ADLS Gen2 linked (from Challenge 1) containing the Open Powerlifting CSV (e.g., `/raw/openpowerlifting.csv`).
- Optional: Additional datasets from Challenge 6 (e.g., BMI for feature enrichment).
- Access to Synapse Studio.
- Basic PySpark knowledge; if not, review with students.
- No prior ML experience needed—focus on concepts like features (inputs like lifts), clustering (grouping similar data points), and evaluation (e.g., silhouette score).

Coaching Tip: If the dataset is too large, suggest sampling (e.g., top 10,000 rows) for faster iteration. Ensure RBAC allows Spark job execution (grant "Synapse Apache Spark Administrator" if needed).

## Step 1: Create and Configure an Apache Spark Pool
- In Synapse Studio: Go to Manage > Apache Spark pools > New.
- **Configurations**:
  - Name: e.g., "PowerliftingMLPool".
  - Node size: Start with Small (4 vCores, 32 GB) for cost efficiency; scale to Medium/Large for full dataset.
  - Autoscale: Enabled, min 3 nodes, max 10 (adjust based on data size).
  - Auto-pause: 15 minutes idle.
  - Spark version: Latest (e.g., 3.4).
  - Dynamic allocation: Enabled for resource optimization.
- **Package Management**: 
  - Under Advanced > Packages: Upload .whl or .jar if needed (e.g., for custom libs). For MLlib, no extras required—it's built-in.
  - For SynapseML (optional bonus): Install via workspace packages or session config. Upload `synapseml-1.0.5-py3-none-any.whl` from Maven (search "synapseml pypi").
  - Requirements.txt: For Python packages, create a file with e.g., `scipy==1.10.1` and upload under Workspace packages.

Explanation: Spark pools provide a serverless, on-demand cluster for big data processing. Configurations balance cost/performance; packages ensure dependencies like MLlib are available.

Coaching Tip: Share screen to create the pool (takes ~5-10 min to provision). Discuss: "Why Spark over SQL?" (ML on large data requires distributed compute.) Test by creating a sample notebook and running `spark.version`.

## Step 2: Create a Spark Notebook and Load Data
- In Synapse Studio: Develop > Notebook > New notebook.
- Attach to your Spark pool.
- Load data from ADLS:
  ```python
  # Cell 1: Load data
  df = spark.read.csv("abfss://powerlifting-data@<storage-account>.dfs.core.windows.net/raw/openpowerlifting.csv", header=True, inferSchema=True)
  
  # Show sample
  df.show(5)
  ```
- Preprocess: Select relevant columns, handle missing values, filter (e.g., only equipped lifts or specific federations).
  ```python
  # Cell 2: Preprocess
  from pyspark.sql.functions import col
  
  # Select performance features (adjust as needed)
  features = ['BodyweightKg', 'Best3SquatKg', 'Best3BenchKg', 'Best3DeadliftKg', 'TotalKg']
  df_ml = df.select(features).na.drop()  # Drop rows with nulls
  
  # Optional: Join with external data (e.g., BMI from Challenge 6)
  # bmi_df = spark.read.csv("abfss://.../raw/external/bmi.csv", header=True)
  # df_ml = df_ml.join(bmi_df, df_ml['Country'] == bmi_df['Country'], 'left')
  
  # Sample for faster testing
  df_ml = df_ml.sample(fraction=0.01)  # 1% sample
  ```

Explanation: Data is loaded as a Spark DataFrame for distributed processing. Features like bodyweight and lifts are key for clustering performance. Dropping nulls ensures clean data; sampling speeds up dev.

Coaching Tip: Run cells interactively. Ask: "What features might define 'raw performance'?" (E.g., totals normalized by bodyweight.) Visualize with `df_ml.toPandas().hist()` if matplotlib is available.

## Step 3: Feature Engineering and Scaling
- Assemble features into a vector (required for MLlib).
- Scale to normalize (clustering is distance-based).
  ```python
  # Cell 3: Feature assembly and scaling
  from pyspark.ml.feature import VectorAssembler, StandardScaler
  
  assembler = VectorAssembler(inputCols=features, outputCol="features")
  df_assembled = assembler.transform(df_ml)
  
  scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures", withStd=True, withMean=True)
  scaler_model = scaler.fit(df_assembled)
  df_scaled = scaler_model.transform(df_assembled)
  ```

Explanation: VectorAssembler combines columns into a single feature vector. StandardScaler centers data to mean 0, std 1—crucial for K-Means to avoid feature dominance (e.g., bodyweight in kg vs. lifts in hundreds).

Coaching Tip: Explain scaling with an analogy: "Like comparing apples to oranges—normalize units." Show before/after with `df_scaled.select('features', 'scaledFeatures').show()`.

## Step 4: Apply Clustering Algorithm
- Use K-Means to cluster.
- Train and predict.
  ```python
  # Cell 4: K-Means clustering
  from pyspark.ml.clustering import KMeans
  from pyspark.ml.evaluation import ClusteringEvaluator
  
  k = 5  # Number of clusters (experiment: try 3-10 for weight classes)
  kmeans = KMeans(featuresCol='scaledFeatures', k=k, seed=42)
  model = kmeans.fit(df_scaled)
  
  # Predict clusters
  predictions = model.transform(df_scaled)
  predictions.select('scaledFeatures', 'prediction').show(10)
  
  # Evaluate (silhouette score: higher = better clusters, closer to 1)
  evaluator = ClusteringEvaluator(featuresCol='scaledFeatures')
  silhouette = evaluator.evaluate(predictions)
  print(f"Silhouette Score: {silhouette}")
  ```

Explanation: K-Means partitions data into K groups by minimizing intra-cluster distances. Here, clusters might represent "performance tiers" or alternative weight classes (e.g., light-strong vs. heavy-moderate). Seed ensures reproducibility.

Coaching Tip: Visualize clusters: Convert to Pandas and plot with matplotlib (e.g., scatter Bodyweight vs. Total, color by prediction). Discuss: "If clusters don't align with official weight classes, what does that suggest?" Experiment with K using elbow method (loop over K, plot cost).

## Step 5: Analyze and Interpret Results
- Group by cluster and compute stats.
  ```python
  # Cell 5: Analyze clusters
  from pyspark.sql.functions import avg, count
  
  cluster_stats = predictions.groupBy('prediction').agg(
      count('*').alias('Count'),
      avg('BodyweightKg').alias('Avg Bodyweight'),
      avg('TotalKg').alias('Avg Total'),
      avg('Best3SquatKg').alias('Avg Squat'),
      avg('Best3BenchKg').alias('Avg Bench'),
      avg('Best3DeadliftKg').alias('Avg Deadlift')
  )
  cluster_stats.show()
  ```
- Interpret: Compare to official powerlifting weight classes (e.g., IPF: <59kg, 59-66kg, etc.). If clusters show better separation by performance, suggest refinements.

Explanation: Stats reveal cluster characteristics (e.g., Cluster 0: Heavy lifters with high totals). Use for insights like "Cluster 2 has high squat/deadlift ratios—possible new class?"

Coaching Tip: Share visualizations (e.g., bar charts of avgs). Ask: "Are there more appropriate weight classes based on this?" (E.g., performance-based vs. bodyweight-only.) Discuss limitations: Overfitting, choice of K, data biases.

## Step 6: Save Results and Clean Up
- Write clusters back to ADLS.
  ```python
  # Cell 6: Save
  predictions.write.mode('overwrite').parquet("abfss://.../processed/clustered_lifters.parquet")
  ```
- Stop the Spark session if done.

Explanation: Parquet is efficient for analytics; enables querying in SQL pools.

Coaching Tip: Emphasize saving for reproducibility. Clean up: Delete Spark pool if not needed to save costs.

## Bonus: Advanced ML with SynapseML
- Install SynapseML via package management (as in Step 1).
- Use for more algorithms, e.g., Gaussian Mixture Model (better for overlapping clusters).
  ```python
  # Example with SynapseML (after import com.microsoft.azure.synapseml)
  # But stick to MLlib for basics.
  ```

Coaching Tip: For advanced students, integrate external data (e.g., cluster by BMI + performance).

## Additional Notes for Coaches
- **Common Pitfalls**: Large data causing OOM—use sampling or larger nodes. Schema inference failing—specify types. Low silhouette (<0.5) indicates poor clustering—tune features/K.
- **Extensions**: Hyperparameter tuning with CrossValidator. Visualize in Power BI (link to Synapse). Predict new lifters' clusters.
- **Resources**: Microsoft Docs: [Spark MLlib](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-machine-learning-mllib-notebooks), [SynapseML](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/synapse-ml-overview). Powerlifting classes: Search IPF/USAPL rules.
- **Assessment**: Students run clustering, interpret 2-3 clusters, and propose if weight classes need adjustment. Share notebook screenshots.

This completes the coach's guide for Challenge 7. Adapt the student guide to action-oriented steps with code templates.
